# -*- coding: utf-8 -*-
"""Code l·∫ßn 4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VSKSWK_iCi7OYyzevFNyi4ksEhCtYpU6

# Ph·∫ßn 1: Ch·ªçn m√¥ h√¨nh

#1.Import th∆∞ vi·ªán
"""

# !pip install pandas numpy matplotlib seaborn lifetimes scikit-learn xgboost tqdm
!pip install lifetimes
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta


from lifetimes import BetaGeoFitter, GammaGammaFitter
from lifetimes.utils import summary_data_from_transaction_data


from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor, XGBClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score, average_precision_score


import warnings
warnings.filterwarnings('ignore')


sns.set_style('whitegrid')
plt.rcParams['figure.dpi']=120

"""#2.Load Data"""

from google.colab import drive
drive.mount('/content/drive')

customers = pd.read_csv("/content/drive/MyDrive/KLTN - Dataset/olist_customers_dataset.csv")
orders = pd.read_csv("/content/drive/MyDrive/KLTN - Dataset/olist_orders_dataset.csv")
items = pd.read_csv("/content/drive/MyDrive/KLTN - Dataset/olist_order_items_dataset.csv")
payments = pd.read_csv("/content/drive/MyDrive/KLTN - Dataset/olist_order_payments_dataset.csv")
reviews = pd.read_csv("/content/drive/MyDrive/KLTN - Dataset/olist_order_reviews_dataset.csv")
#geo = pd.read_csv("/content/drive/MyDrive/KLTN - Dataset/olist_geolocation_dataset.csv")
#product = pd.read_csv("/content/drive/MyDrive/KLTN - Dataset/olist_products_dataset.csv")

# Ki·ªÉm tra nhanh k√≠ch th∆∞·ªõc v√† c·ªôt
print(customers.shape, orders.shape, items.shape, payments.shape, reviews.shape)
orders.head(3)

"""## 2.1. Table orders"""

orders.head(4)

orders.info()

#T·∫°o funtion ƒë·ªÉ t√≠nh t·ªâ l·ªá gi√° tr·ªã b·ªã null trong c√°c h√†ng
def calc_null_rate(df):
  newdf=df.isnull().sum().to_frame('null_count')
  newdf[['null_rate']]=newdf[['null_count']]/len(df)
  return newdf.sort_values(by=['null_rate'],ascending=False)

calc_null_rate(orders)

"""* Nh·∫≠n x√©t: 3 c·ªôt ƒë·∫ßu ti√™n c√≥ gi√° tr·ªã null nh∆∞ng ph·∫ßn ph√¢n t√≠ch ph√≠a d∆∞·ªõi kh√¥ng d√πng n√™n s·∫Ω l∆∞·ª£c b·ªè"""

orders['order_status'].value_counts(normalize=True) * 100

"""* L∆∞u √Ω: ·ªû ph·∫ßn ph√¢n t√≠ch ph√≠a sau, ch·ªâ gi·ªØ l·∫°i c√°c ƒë∆°n h√†ng ƒë√£ c√≥ tr·∫°ng th√°i chuy·ªÉn "delivered"

## 2.2. Table items
"""

items.head(4)

items.head(4)

items.info()

calc_null_rate(items)

items['order_id'].nunique()

"""Nh·∫≠n x√©t: V√¨ m·ªói ƒë∆°n h√†ng c√≥ nhi·ªÅu order_item kh√°c nhau"""

items.describe()

"""+ Nh·∫≠n x√©t: C·ªôt price v√† freight value kh√¥ng xu·∫•t hi·ªán gi√° tr·ªã √¢m => Ph√π h·ª£p"""

# Ki·ªÉm tra t·ª∑ l·ªá freight_value = 0
zero_freight = (items['freight_value'] == 0).sum()
total_rows = len(items)
print(f"S·ªë d√≤ng freight_value = 0: {zero_freight} ({zero_freight/total_rows*100:.2f}%)")

"""+ Nh·∫≠n x√©t: C√≥ th·ªÉ ƒë√¢y l√† nh·ªØng ƒë∆°n h√†ng freeship n√™n gi√° tr·ªã v·∫≠n chuy·ªÉn = 0 l√† b√¨nh th∆∞·ªùng

## 2.3. Table customers
"""

customers.head(4)

customers.info()

customers['customer_id'].nunique()

customers['customer_unique_id'].nunique()

"""* Nh·∫≠n x√©t:
  + C√≥ 99,441 ƒë∆°n h√†ng ƒë∆∞·ª£c g√°n cho 96,096 kh√°ch h√†ng th·ª±c t·∫ø
  ‚Üí T·ª©c l√† ch·ªâ kho·∫£ng 3,345 kh√°ch h√†ng ƒë√£ mua t·ª´ 2 l·∫ßn tr·ªü l√™n
"""

repeat_customers = (
    customers.groupby('customer_unique_id')['customer_id']
    .nunique()
    .reset_index(name='order_count')
)

repeat_customers['is_repeat'] = repeat_customers['order_count'] > 1
repeat_customers['is_repeat'].value_counts(normalize=True) * 100

"""‚Üí Nghƒ©a l√† kho·∫£ng 3.12% kh√°ch h√†ng th·ª±c ƒë√£ mua nhi·ªÅu h∆°n 1 l·∫ßn v√† g·∫ßn 97% kh√°ch h√†ng ch·ªâ mua 1 l·∫ßn duy nh·∫•t"""

# Check qua s·ªë kh√°ch h√†ng ·ªü m·ªói th√°ng

# G·ªôp 2 b·∫£ng qua kh√≥a customer_id
orders_cust = orders.merge(customers, on='customer_id', how='left')

# Chuy·ªÉn c·ªôt th·ªùi gian sang datetime
orders_cust['order_date'] = pd.to_datetime(orders_cust['order_purchase_timestamp'])

# T·∫°o c·ªôt th√°ng
orders_cust['order_month'] = orders_cust['order_date'].dt.to_period('M')

# T√≠nh s·ªë kh√°ch h√†ng unique m·ªói th√°ng (d·ª±a tr√™n customer_unique_id)
unique_users_by_month = (
    orders_cust.groupby('order_month')['customer_unique_id']
    .nunique()
    .reset_index()
    .rename(columns={'customer_unique_id': 'n_unique_users'})
)

# Chuy·ªÉn order_month v·ªÅ d·∫°ng timestamp
unique_users_by_month['order_month'] = unique_users_by_month['order_month'].dt.to_timestamp()

# In k·∫øt qu·∫£
print(unique_users_by_month.head(30))

"""* Nh·∫≠n x√©t: M·∫∑c d√π Dataset t·ª´ 9/2016 - 10/2018, tuy nhi√™n b·ªã thi·∫øu d·ªØ li·ªáu c·ªßa th√°ng 11/2016, ƒë·ªìng th·ªùi s·ªë kh√°ch h√†ng c·ªßa th√°ng 9,10,12 nƒÉm 2016 v√† th√°ng 9,10 c·ªßa nƒÉm 2018 r·∫•t nh·ªè
  * V√¨ th·∫ø, b√†i ph√¢n t√≠ch ch·ªâ t·∫≠p trung v√†o kho·∫£ng 1/2017 ƒë·∫øn 6/2018

#3.G·ªôp b·∫£ng v√† l·ªçc ƒëi·ªÅu ki·ªán
"""

# Gi·ªØ ƒë∆°n ƒë√£ giao th√†nh c√¥ng
orders = orders[orders['order_status'] == 'delivered'].copy()
orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'], errors='coerce')

# T√≠nh gi√° tr·ªã ƒë∆°n h√†ng = price + freight_value
items['order_value'] = items['price'] + items['freight_value']
order_val = items.groupby('order_id', as_index=False)['order_value'].sum()

# Merge b·∫£ng giao d·ªãch c∆° b·∫£n (1 d√≤ng = 1 order)
trans = (
    orders[['order_id', 'customer_id', 'order_purchase_timestamp']]
    .merge(order_val, on='order_id', how='left')
    .merge(customers[['customer_id', 'customer_unique_id', 'customer_state']], on='customer_id', how='left')
    .rename(columns={'order_purchase_timestamp': 'order_date'})
    [['order_id', 'customer_unique_id', 'order_date', 'order_value', 'customer_state']]
    .dropna()
    .drop_duplicates()
)
trans['order_date'] = pd.to_datetime(trans['order_date'], errors='coerce')

# Gi·ªØ l·∫°i d·ªØ li·ªáu t·ª´ 01/01/2017 tr·ªü ƒëi
trans = trans[trans['order_date'] >= '2017-01-01']
print("T·ªïng s·ªë giao d·ªãch:", len(trans))
trans.head(3)

trans.info()

calc_null_rate(trans)

"""#4.EDA (ph√¢n ph·ªëi & ƒë·∫∑c ƒëi·ªÉm d·ªØ li·ªáu)"""

plt.figure(figsize=(8,4))
sns.histplot(trans['order_value'], bins=80, log_scale=True)
plt.title("Ph√¢n b·ªë gi√° tr·ªã ƒë∆°n h√†ng (log scale)")
plt.xlabel("Order Value (log)")
plt.ylabel("S·ªë l∆∞·ª£ng"); plt.show()

# Ph√¢n ph·ªëi s·ªë ƒë∆°n theo kh√°ch
cust_orders = trans.groupby('customer_unique_id')['order_id'].nunique()
sns.histplot(cust_orders, bins=40)
plt.title("S·ªë ƒë∆°n h√†ng m·ªói kh√°ch"); plt.xlabel("S·ªë ƒë∆°n"); plt.show()
print("T·ª∑ l·ªá kh√°ch ch·ªâ mua 1 l·∫ßn:", (cust_orders.eq(1).mean()*100).round(2), "%")

# T·ª∑ tr·ªçng ph∆∞∆°ng th·ª©c thanh to√°n
pm = payments.merge(orders[['order_id']], on='order_id', how='inner')
share = pm['payment_type'].value_counts(normalize=True).head(6)
share.plot(kind='bar', figsize=(8,4))
plt.title("T·ª∑ tr·ªçng ph∆∞∆°ng th·ª©c thanh to√°n (Top)"); plt.ylabel("T·ª∑ l·ªá"); plt.show()

"""#5.T√°ch th·ªùi gian (Cutoff & Horizon) + Chia Train/Test"""

# Thi·∫øt l·∫≠p m·ªëc th·ªùi gian t√°ch d·ªØ li·ªáu (temporal split)
# - CUTOFF: m·ªëc t√°ch train/test (31/12/2017)
# - H_START, H_END: giai ƒëo·∫°n d·ª± b√°o CLV (01/2018‚Äì06/2018)
# - SNAPSHOT: ƒëi·ªÉm tham chi·∫øu t√≠nh Recency
CUTOFF  = pd.Timestamp('2017-12-31')
H_START = CUTOFF + pd.Timedelta(days=1)
H_END   = H_START + pd.DateOffset(months=6)
SNAPSHOT = CUTOFF + pd.Timedelta(days=1)

# Chia d·ªØ li·ªáu:
# - train_tx: giao d·ªãch tr∆∞·ªõc cutoff (d√πng hu·∫•n luy·ªán)
# - test_tx : giao d·ªãch sau cutoff (d√πng ƒë√°nh gi√° CLV th·ª±c t·∫ø)
train_tx = trans[trans['order_date'] <= CUTOFF].copy()
test_tx  = trans[(trans['order_date'] > CUTOFF) & (trans['order_date'] <= H_END)].copy()

# ‚ö†Ô∏è L∆∞u √Ω: Kh√°ch h√†ng ch·ªâ xu·∫•t hi·ªán sau cutoff (cold-start) s·∫Ω b·ªã lo·∫°i t·ª± ƒë·ªông
# v√¨ kh√¥ng c√≥ l·ªãch s·ª≠ trong train_tx ‚Üí kh√¥ng th·ªÉ d·ª± b√°o CLV
print("Train orders:", len(train_tx), "| Test (horizon):", len(test_tx))

# Minh h·ªça tr·ª±c quan Cutoff & Horizon
plt.figure(figsize=(10,2))
plt.axvline(CUTOFF, color='r', linestyle='--', label='Cutoff (31/12/2017)')
plt.axvspan(train_tx['order_date'].min(), CUTOFF, color='lightblue', alpha=0.5, label='Train')
plt.axvspan(H_START, H_END, color='lightgreen', alpha=0.4, label='Horizon 6M')
plt.title("Minh h·ªça t√°ch th·ªùi gian (Cutoff & Horizon)")
plt.legend(); plt.tight_layout(); plt.show()

train_tx['customer_unique_id'].nunique()

train_tx.info()

"""#6.T√≠nh RFM (Recency‚ÄìFrequency‚ÄìMonetary)"""

rfm = (
    train_tx.groupby('customer_unique_id')
    .agg(last=('order_date', 'max'),
         first=('order_date', 'min'),
         num=('order_id', 'nunique'),
         spent=('order_value', 'sum'))
)
rfm['recency']   = (SNAPSHOT - rfm['last']).dt.days
rfm['T']         = (SNAPSHOT - rfm['first']).dt.days
rfm['frequency'] = rfm['num'] - 1
rfm['monetary']  = rfm['spent'] / rfm['num']

rfm_simple = rfm[['recency', 'frequency', 'monetary']].reset_index()
print("S·ªë kh√°ch h√†ng:", len(rfm_simple))
rfm_simple.head(3)

import pandas as pd, numpy as np, matplotlib.pyplot as plt

# Nh√≥m frequency th√†nh 5 m·ª©c: 0,1,2,3,>4
bins = [-1, 0, 1, 2, 3, np.inf]
labels = ['0', '1', '2', '3', '>4']
rfm_simple['freq_group'] = pd.cut(rfm_simple['frequency'], bins=bins, labels=labels)

# ƒê·∫øm s·ªë kh√°ch h√†ng v√† t·ª∑ l·ªá %
freq_summary = (
    rfm_simple['freq_group']
    .value_counts()
    .sort_index()
    .reset_index()
    .rename(columns={'index': 'frequency_group', 'freq_group': 'count'})  # s·ª≠a ·ªü ƒë√¢y
)

# üõ†Ô∏è S·ª≠a t√™n ƒë√∫ng c·ªßa c·ªôt ƒë·∫øm
freq_summary.columns = ['frequency_group', 'count']

# √âp ki·ªÉu sang int
freq_summary['count'] = freq_summary['count'].astype(int)

# Th√™m c·ªôt % kh√°ch h√†ng
freq_summary['percent'] = (freq_summary['count'] / freq_summary['count'].sum() * 100).round(2)

print("\nüìä Ph√¢n ph·ªëi t·∫ßn su·∫•t mua h√†ng (Frequency):")
print(freq_summary.to_string(index=False))

# (T√πy ch·ªçn) V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(6,3))
plt.bar(freq_summary['frequency_group'], freq_summary['count'], color='#42A5F5')
plt.xlabel("S·ªë l·∫ßn mua (Frequency)")
plt.ylabel("S·ªë kh√°ch h√†ng")
plt.title("Ph√¢n ph·ªëi t·∫ßn su·∫•t mua h√†ng (Train set)")
for i, v in enumerate(freq_summary['count']):
    plt.text(i, v + 5, f"{freq_summary['percent'][i]}%", ha='center', fontsize=9)
plt.tight_layout()
plt.show()

"""# Th√™m"""

# 2Ô∏è‚É£ T√≠nh RFM kh√¥ng tr·ª´ 1 (d√πng cho ph√¢n t√≠ch h√†nh vi)
rfm_nominus = (
    train_tx.groupby('customer_unique_id')
    .agg(last=('order_date', 'max'),
         first=('order_date', 'min'),
         num=('order_id', 'nunique'),
         spent=('order_value', 'sum'))
)

rfm_nominus['recency']   = (SNAPSHOT - rfm_nominus['last']).dt.days
rfm_nominus['T']         = (SNAPSHOT - rfm_nominus['first']).dt.days
rfm_nominus['frequency'] = rfm_nominus['num']      # kh√¥ng tr·ª´ 1
rfm_nominus['monetary']  = rfm_nominus['spent'] / rfm_nominus['num']

rfm_nominus = rfm_nominus[['recency', 'frequency', 'T', 'monetary']].reset_index()
rfm_nominus['type'] = 'RFM_raw'

import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(1, 2, figsize=(12,5))

sns.histplot(rfm_nominus['frequency'], bins=30, ax=axes[0], color='skyblue', kde=True)
axes[0].set_title("Frequency (kh√¥ng tr·ª´ 1)")

sns.histplot(rfm_std['frequency'], bins=30, ax=axes[1], color='salmon', kde=True)
axes[1].set_title("Frequency (tr·ª´ 1)")

for ax in axes:
    ax.set_xlabel("Frequency")
    ax.set_ylabel("S·ªë kh√°ch h√†ng")

plt.suptitle("So s√°nh ph√¢n ph·ªëi Frequency gi·ªØa hai c√°ch t√≠nh RFM", fontsize=13)
plt.tight_layout()
plt.show()

"""#7.Cohort Retention"""

train_tx['order_month'] = train_tx['order_date'].dt.to_period('M')
first_month = train_tx.groupby('customer_unique_id')['order_month'].min().rename('cohort')
cohort_df = train_tx.merge(first_month, on='customer_unique_id', how='left')
cohort_df['cohort_index'] = (cohort_df['order_month'] - cohort_df['cohort']).apply(lambda p: p.n)

cohort_pivot = (
    cohort_df.groupby(['cohort', 'cohort_index'])['customer_unique_id']
    .nunique().unstack(1).fillna(0)
)
retention = cohort_pivot.divide(cohort_pivot.iloc[:,0], axis=0)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# B·ªè c·ªôt 0 (th√°ng ƒë·∫ßu = 100%)
sns.heatmap(
    retention.iloc[:, 1:13],    # ch·ªâ l·∫•y t·ª´ th√°ng 1 ƒë·∫øn th√°ng 12
    annot=True,
    fmt=".2%",
    cmap="YlGnBu",
    cbar=True
)

plt.title("Cohort Retention (Train-only) ‚Äî 12 th√°ng ƒë·∫ßu (·∫©n c·ªôt 100%)", fontsize=13)
plt.ylabel("Cohort theo th√°ng ƒë·∫ßu")
plt.xlabel("Th√°ng k·ªÉ t·ª´ th√°ng ƒë·∫ßu ti√™n")
plt.show()

"""* Nh·∫≠n x√©t t·ªïng quan:
  - T·ª∑ l·ªá quay l·∫°i nh√¨n chung r·∫•t th·∫•p, ch·ªâ dao ƒë·ªông quanh 0.2‚Äì0.7% ·ªü th√°ng k·∫ø ti·∫øp, r·ªìi gi·∫£m nhanh v·ªÅ g·∫ßn 0% sau 3‚Äì4 th√°ng.

  - ƒêa s·ªë cohort ƒë·ªÅu c√≥ hi·ªán t∆∞·ª£ng ‚Äúr∆°i r·ª•ng nhanh‚Äù, ƒë·∫∑c bi·ªát t·ª´ th√°ng th·ª© 4 tr·ªü ƒëi, g·∫ßn nh∆∞ kh√¥ng c√≤n kh√°ch quay l·∫°i.

  - ƒêi·ªÅu n√†y cho th·∫•y ph·∫ßn l·ªõn kh√°ch h√†ng ch·ªâ ph√°t sinh m·ªôt l·∫ßn mua, ƒë√∫ng v·ªõi m√¥ t·∫£ ‚Äúone-time heavy‚Äù th∆∞·ªùng th·∫•y trong th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ marketplace.

#8.Baseline CLV ‚Äì Fit BG/NBD & Gamma‚ÄìGamma (Feature Engineering II)
"""

from lifetimes import BetaGeoFitter, GammaGammaFitter
from lifetimes.utils import summary_data_from_transaction_data

# T·∫°o b·∫£ng t√≥m t·∫Øt h√†nh vi mua (input cho lifetimes)
summary = summary_data_from_transaction_data(
    transactions=train_tx,
    customer_id_col='customer_unique_id',
    datetime_col='order_date',
    monetary_value_col='order_value',
    observation_period_end=SNAPSHOT,
    freq='D'
).reset_index()

# Fit m√¥ h√¨nh BG/NBD (t·∫ßn su·∫•t mua)
bgf = BetaGeoFitter(penalizer_coef=0.001)
bgf.fit(summary['frequency'], summary['recency'], summary['T'])

# Fit m√¥ h√¨nh Gamma‚ÄìGamma (chi ti√™u trung b√¨nh)
mask_rep = (summary['frequency'] > 0) & (summary['monetary_value'] > 0)
ggf = GammaGammaFitter(penalizer_coef=0.001)
ggf.fit(summary.loc[mask_rep,'frequency'], summary.loc[mask_rep,'monetary_value'])

# D·ª± b√°o s·ªë l·∫ßn mua v√† chi ti√™u trong 6 th√°ng t·ªõi
H_days = (H_END - H_START).days + 1
summary['pred_purchases_6m'] = bgf.predict(H_days, summary['frequency'], summary['recency'], summary['T'])
summary['pred_avg_value']    = ggf.conditional_expected_average_profit(summary['frequency'], summary['monetary_value'])
summary['CLV_prob_6m']       = summary['pred_purchases_6m'] * summary['pred_avg_value']

# X√°c su·∫•t c√≤n ho·∫°t ƒë·ªông
summary['p_alive'] = bgf.conditional_probability_alive(summary['frequency'], summary['recency'], summary['T'])

# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi CLV d·ª± b√°o
plt.figure(figsize=(8,4))
sns.histplot(summary['CLV_prob_6m'], bins=60, kde=True)
plt.title('Ph√¢n b·ªë CLV d·ª± b√°o (BG/NBD + Gamma‚ÄìGamma)')
plt.xlabel('CLV_prob_6m'); plt.show()

"""* Nh·∫≠n x√©t:
  - Ph√¢n ph·ªëi CLV d·ª± b√°o (BG/NBD + Gamma‚ÄìGamma) cho th·∫•y gi√° tr·ªã r·∫•t l·ªách ph·∫£i, ph·∫ßn l·ªõn kh√°ch h√†ng c√≥ CLV th·∫•p trong khi ch·ªâ m·ªôt s·ªë √≠t c√≥ gi√° tr·ªã cao v∆∞·ª£t tr·ªôi --> Ph·∫£n √°nh s·ª± t·∫≠p trung gi√° tr·ªã trong nh√≥m nh·ªè kh√°ch h√†ng
"""

import matplotlib.ticker as mtick

# --- Bi·ªÉu ƒë·ªì 1: To√†n b·ªô ph√¢n ph·ªëi CLV (log scale ƒë·ªÉ x·ª≠ l√Ω heavy-tail) ---
plt.figure(figsize=(8,4))
sns.histplot(summary['CLV_prob_6m'], bins=80, kde=True, color='#66b3ff', alpha=0.8)
plt.xscale('log')  # log scale gi√∫p ‚Äún√©n‚Äù ph·∫ßn ƒëu√¥i d√†i
plt.title('Ph√¢n b·ªë CLV d·ª± b√°o (BG/NBD + Gamma‚ÄìGamma) ‚Äî log scale')
plt.xlabel('CLV_prob_6m (log scale)')
plt.ylabel('S·ªë l∆∞·ª£ng kh√°ch h√†ng')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# --- Bi·ªÉu ƒë·ªì 2: Zoom ph·∫ßn CLV nh·ªè (‚â§ 100) ƒë·ªÉ xem r√µ ‚Äúph·ªï ƒë·∫ßu‚Äù ---
plt.figure(figsize=(8,4))
sns.histplot(summary.loc[summary['CLV_prob_6m'] <= 100, 'CLV_prob_6m'],
             bins=40, kde=True, color='#ff9999', alpha=0.8)
plt.title('Ph√¢n b·ªë CLV d·ª± b√°o (BG/NBD + Gamma‚ÄìGamma) ‚Äî Zoom ‚â§ 100')
plt.xlabel('CLV_prob_6m')
plt.ylabel('S·ªë l∆∞·ª£ng kh√°ch h√†ng')
plt.grid(alpha=0.3)

# Th√™m label top gi√° tr·ªã (nh∆∞ 0‚Äì20, 20‚Äì40,‚Ä¶)
plt.gca().xaxis.set_major_formatter(mtick.StrMethodFormatter('{x:,.0f}'))
plt.tight_layout()
plt.show()

# --- Th√™m bi·ªÉu ƒë·ªì 3: Boxplot ƒë·ªÉ th·∫•y to√†n c·∫£nh ‚Äúph√¢n ph·ªëi l·ªách‚Äù ---
plt.figure(figsize=(7,2))
sns.boxplot(x=summary['CLV_prob_6m'], color='#99c2ff')
plt.title('Boxplot CLV d·ª± b√°o ‚Äî th·∫•y r√µ outliers v√† ph√¢n ph·ªëi l·ªách')
plt.xlabel('CLV_prob_6m')
plt.tight_layout()
plt.show()

"""#9.G·∫Øn nh√£n CLV th·ª±c t·∫ø & t·∫°o Feature Set"""

# Nh√£n th·∫≠t (chi ti√™u th·ª±c t·∫ø trong Horizon) - Horizon l√† 6 th√°ng ƒë·∫ßu c·ªßa 2018
y_true = (
    test_tx.groupby('customer_unique_id')['order_value'].sum()
    .rename('CLV_actual_6m')
)
summary = summary.merge(y_true, on='customer_unique_id', how='left')
summary['CLV_actual_6m'] = summary['CLV_actual_6m'].fillna(0.0)

# B·ªï sung feature h√†nh vi m·ªü r·ªông (payment/review/items)
agg_base = train_tx[['order_id','customer_unique_id','customer_state']].copy()
feat_aggs = (
    agg_base
    .merge(payments[['order_id','payment_value']], on='order_id', how='left')
    .merge(reviews[['order_id','review_score']], on='order_id', how='left')
    .merge(items[['order_id','product_id']], on='order_id', how='left')
    .groupby('customer_unique_id')
    .agg(
        payment_value_sum   = ('payment_value','sum'),
        payment_value_mean  = ('payment_value','mean'),
        review_score_mean   = ('review_score','mean'),
        num_unique_products = ('product_id', pd.Series.nunique),
        state_mode          = ('customer_state', lambda x: x.mode()[0] if len(x.mode())>0 else np.nan)
    ).reset_index()
)

# Merge features v√†o b·∫£ng summary
summary_ml = summary.merge(feat_aggs, on='customer_unique_id', how='left')
summary_ml.head(5)

calc_null_rate(summary_ml)

# X·ª≠ l√Ω thi·∫øu
# Ch·ªâ c√≤n c·ªôt 'review_score_mean' b·ªã thi·∫øu (~0.7% kh√°ch)
# ƒêi·ªÅn gi√° tr·ªã trung v·ªã ƒë·ªÉ gi·ªØ ph√¢n ph·ªëi ·ªïn ƒë·ªãnh, tr√°nh m·∫•t m·∫´u
median_review = summary_ml['review_score_mean'].median()
summary_ml['review_score_mean'] = summary_ml['review_score_mean'].fillna(median_review)

print("T·ªïng s·ªë kh√°ch h√†ng:", len(summary_ml))
summary_ml.head(3)

"""#10.T·∫°o Preprocessor v√† 3 Pipeline ·ª®ng vi√™n"""

from sklearn.model_selection import KFold, cross_val_score

# X√°c ƒë·ªãnh c·ªôt numeric v√† categorical
numeric_features = [
    'frequency','recency','T','monetary_value',
    'p_alive','CLV_prob_6m',
    'payment_value_sum','payment_value_mean',
    'review_score_mean','num_unique_products'
]
categorical_features = ['state_mode']

# Preprocessor: scale s·ªë + OneHotEncode h·∫°ng m·ª•c
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ]
)

# Chu·∫©n b·ªã t·∫≠p X, y
X_final = summary_ml[numeric_features + categorical_features].copy()
y_final = summary_ml['CLV_actual_6m'].astype(float).copy()

# ƒê·ªãnh nghƒ©a 3 m√¥ h√¨nh
models = {
    "LinearRegression": LinearRegression(),
    "RandomForest": RandomForestRegressor(n_estimators=400, max_depth=12, random_state=42),
    "XGBoost": XGBRegressor(
        n_estimators=700, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8, tree_method='hist', random_state=42
    )
}

# T·∫°o Pipeline cho t·ª´ng model
from sklearn.pipeline import Pipeline
pipelines = {
    name: Pipeline([('preprocess', preprocessor), ('model', model)])
    for name, model in models.items()
}

"""#11.Cross-Validation & L·ª±a ch·ªçn M√¥ h√¨nh T·ªët nh·∫•t

## 11.1 So s√°nh m√¥ h√¨nh tr√™n to√†n b·ªô t·∫≠p train
  * M·ª•c ƒë√≠ch:

    - Ch·∫°y 3 m√¥ h√¨nh kh√°c nhau (Linear, RF, XGB) tr√™n c√πng d·ªØ li·ªáu.

    - ƒêo l∆∞·ªùng ƒë·ªô kh·ªõp (fit) tr√™n t·∫≠p hu·∫•n luy·ªán (X_final, y_final).

    - Gi√∫p so s√°nh nhanh m√¥ h√¨nh n√†o c√≥ hi·ªáu nƒÉng t·ªïng th·ªÉ t·ªët h∆°n tr∆∞·ªõc khi ƒëi v√†o ƒë√°nh gi√° ch√≠nh th·ª©c.
"""

for model_name in ["LinearRegression", "RandomForest", "XGBoost"]:
    pipe = pipelines[model_name]
    pipe.fit(X_final, y_final)
    y_pred = pipe.predict(X_final)
    print(f"{model_name} ‚Üí MAE={mean_absolute_error(y_final, y_pred):.3f}, "
          f"RMSE={np.sqrt(mean_squared_error(y_final, y_pred)):.3f}, "
          f"R¬≤={r2_score(y_final, y_pred):.3f}")

"""* Nh·∫≠n x√©t: Nh·∫≠n th·∫•y XGBoost cho MAE th·∫•p h∆°n v√† R^2 cao nh·∫•t trong 3 c√°i

## 11.2 Ki·ªÉm ƒë·ªãnh ƒë·ªô ·ªïn ƒë·ªãnh b·∫±ng Cross-Validation (CV)
* M·ª•c ti√™u: ƒê√°nh gi√° ƒë·ªô ·ªïn ƒë·ªãnh v√† kh·∫£ nƒÉng t·ªïng qu√°t c·ªßa m√¥ h√¨nh
    * Thay v√¨ d√πng to√†n b·ªô d·ªØ li·ªáu 1 l·∫ßn, chia l√†m 3 ph·∫ßn (3-Fold CV):

        + m·ªói l·∫ßn 2 ph·∫ßn train, 1 ph·∫ßn test,

        + l·∫∑p l·∫°i 3 l·∫ßn, l·∫•y trung b√¨nh k·∫øt qu·∫£.
"""

pipe_lr = pipelines["LinearRegression"]

from sklearn.model_selection import KFold, cross_val_score
cv = KFold(n_splits=3, shuffle=True, random_state=42)

mae_scores  = -cross_val_score(pipe_lr, X_final, y_final, cv=cv, scoring='neg_mean_absolute_error')
rmse_scores = np.sqrt(-cross_val_score(pipe_lr, X_final, y_final, cv=cv, scoring='neg_mean_squared_error'))
r2_scores   = cross_val_score(pipe_lr, X_final, y_final, cv=cv, scoring='r2')

print("=== Linear Regression (3-Fold CV) ===")
print(f"MAE mean = {mae_scores.mean():.3f} ¬± {mae_scores.std():.3f}")
print(f"RMSE mean = {rmse_scores.mean():.3f} ¬± {rmse_scores.std():.3f}")
print(f"R¬≤ mean = {r2_scores.mean():.3f} ¬± {r2_scores.std():.3f}")

pipe_xgb = pipelines["XGBoost"]

from sklearn.model_selection import KFold, cross_val_score
cv = KFold(n_splits=3, shuffle=True, random_state=42)

mae_scores  = -cross_val_score(pipe_xgb, X_final, y_final, cv=cv, scoring='neg_mean_absolute_error')
rmse_scores = np.sqrt(-cross_val_score(pipe_xgb, X_final, y_final, cv=cv, scoring='neg_mean_squared_error'))
r2_scores   = cross_val_score(pipe_xgb, X_final, y_final, cv=cv, scoring='r2')

print("=== XGBoost (3-Fold CV) ===")
print(f"MAE mean = {mae_scores.mean():.3f} ¬± {mae_scores.std():.3f}")
print(f"RMSE mean = {rmse_scores.mean():.3f} ¬± {rmse_scores.std():.3f}")
print(f"R¬≤ mean = {r2_scores.mean():.3f} ¬± {r2_scores.std():.3f}")

pipe_rf = pipelines["RandomForest"].set_params(model__n_estimators=200)
 from sklearn.model_selection import KFold, cross_val_score
 cv = KFold(n_splits=3, shuffle=True, random_state=42)
 mae_scores = -cross_val_score(pipe_rf, X_final, y_final, cv=cv, scoring='neg_mean_absolute_error')
 rmse_scores = np.sqrt(-cross_val_score(pipe_rf, X_final, y_final, cv=cv, scoring='neg_mean_squared_error'))
 r2_scores = cross_val_score(pipe_rf, X_final, y_final, cv=cv, scoring='r2')
 print("=== Random Forest (3-Fold CV) ===")
 print(f"MAE mean = {mae_scores.mean():.3f} ¬± {mae_scores.std():.3f}")
 print(f"RMSE mean = {rmse_scores.mean():.3f} ¬± {rmse_scores.std():.3f}")
 print(f"R¬≤ mean = {r2_scores.mean():.3f} ¬± {r2_scores.std():.3f}")

"""### T·ªïng k·∫øt ch·ªçn m√¥ h√¨nh:
+ M√¥ h√¨nh Linear Regression c√≥ sai s·ªë trung b√¨nh (MAE, RMSE) th·∫•p nh·∫•t v√† ·ªïn ƒë·ªãnh nh·∫•t,
+ Trong khi Random Forest v√† XGBoost kh√¥ng mang l·∫°i c·∫£i thi·ªán ƒë√°ng k·ªÉ v·ªÅ ƒë·ªô ch√≠nh x√°c.
+ Tuy nhi√™n, do ƒë·∫∑c tr∆∞ng d·ªØ li·ªáu CLV phi tuy·∫øn v√† ph√¢n ph·ªëi l·ªách m·∫°nh, m√¥ h√¨nh phi tuy·∫øn (XGBoost) v·∫´n ƒë∆∞·ª£c gi·ªØ l·∫°i nh∆∞ m√¥ h√¨nh m·ªü r·ªông ƒë·ªÉ khai th√°c ti·ªÅm nƒÉng h·ªçc t∆∞∆°ng t√°c gi·ªØa c√°c ƒë·∫∑c tr∆∞ng.
    + V√¨ v·∫≠y, nghi√™n c·ª©u l·ª±a ch·ªçn Linear Regression l√†m baseline v√† XGBoost l√†m m√¥ h√¨nh cu·ªëi c√πng (final CLV model) cho giai ƒëo·∫°n ph√¢n t√≠ch, ph√¢n kh√∫c v√† ·ª©ng d·ª•ng k·∫øt qu·∫£.

#12.Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost Final
"""

from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# --- L·∫•y l·∫°i preprocessor & c·∫•u h√¨nh model t·ªët nh·∫•t ---
xgb_final = XGBRegressor(
    n_estimators=700,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    tree_method='hist',
    random_state=42
)

pipe_xgb_final = Pipeline([
    ('preprocess', preprocessor),
    ('model', xgb_final)
])

# --- Fit m√¥ h√¨nh tr√™n to√†n b·ªô d·ªØ li·ªáu ---
print("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh XGBoost FINAL tr√™n to√†n b·ªô d·ªØ li·ªáu ...")
pipe_xgb_final.fit(X_final, y_final)

# --- L∆∞u pipeline ƒë√£ hu·∫•n luy·ªán ---
joblib.dump(pipe_xgb_final, 'clv_pipeline_xgb.joblib')
print("ƒê√£ l∆∞u m√¥ h√¨nh pipeline t·∫°i: clv_pipeline_xgb.joblib")

"""#13.D·ª± b√°o CLV Final & ƒê√°nh gi√°"""

# D·ª± b√°o CLV cho to√†n b·ªô kh√°ch h√†ng
y_pred_final = pipe_xgb_final.predict(X_final)

# T√≠nh sai s·ªë & R¬≤
mae_final  = mean_absolute_error(y_final, y_pred_final)
rmse_final = np.sqrt(mean_squared_error(y_final, y_pred_final))
r2_final   = r2_score(y_final, y_pred_final)

print("=== K·∫øt qu·∫£ m√¥ h√¨nh XGBoost FINAL ===")
print(f"MAE = {mae_final:.3f}")
print(f"RMSE = {rmse_final:.3f}")
print(f"R¬≤ = {r2_final:.3f}")

"""#14.Ph√¢n t√≠ch t·∫ßm quan tr·ªçng ƒë·∫∑c tr∆∞ng (Feature Importance)"""

# --- L·∫•y t√™n c·ªôt sau khi qua preprocessor ---
# L·∫•y t√™n c·ªôt numeric
num_names = numeric_features

# L·∫•y t√™n c·ªôt OneHotEncode (categorical)
cat_names = list(
    pipe_xgb_final.named_steps['preprocess']
    .named_transformers_['cat']
    .get_feature_names_out(categorical_features)
)

# G·ªôp l·∫°i
all_feature_names = num_names + cat_names

# --- L·∫•y importance t·ª´ model ---
feature_importance = pipe_xgb_final.named_steps['model'].feature_importances_

# --- T·∫°o b·∫£ng importance ---
imp_df = pd.Series(feature_importance, index=all_feature_names).sort_values(ascending=True)

plt.figure(figsize=(8,6))
imp_df.tail(15).plot(kind='barh')   # ch·ªâ hi·ªÉn th·ªã top 15 feature quan tr·ªçng
plt.title("ƒê·ªô quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng ‚Äî XGBoost FINAL")
plt.xlabel("Feature Importance")
plt.tight_layout()
plt.show()

"""#15.So s√°nh th·ª±c t·∫ø vs d·ª± b√°o & Ph√¢n kh√∫c kh√°ch h√†ng"""

# Th√™m v√†o b·∫£ng t·ªïng h·ª£p
summary_ml['CLV_pred_final'] = np.maximum(y_pred_final, 0)

# V·∫Ω scatter ƒë·ªÉ so s√°nh
plt.figure(figsize=(6,6))
plt.scatter(y_final, y_pred_final, alpha=0.3)
lim = [0, max(y_final.max(), y_pred_final.max())]
plt.plot(lim, lim, 'r--', linewidth=1)
plt.xlim(lim); plt.ylim(lim)
plt.xlabel('CLV th·ª±c (6 th√°ng)')
plt.ylabel('CLV d·ª± b√°o (XGBoost Final)')
plt.title('Th·ª±c t·∫ø vs D·ª± b√°o ‚Äî XGBoost FINAL')
plt.tight_layout(); plt.show()

# --- Ph√¢n kh√∫c theo quantile 33/66 ---
q33, q66 = np.quantile(summary_ml['CLV_pred_final'], [0.33, 0.66])

def seg(v):
    if v <= q33: return 'Low'
    elif v <= q66: return 'Mid'
    else: return 'High'

summary_ml['Segment'] = summary_ml['CLV_pred_final'].apply(seg)

# Th·ªëng k√™ t·ª´ng ph√¢n kh√∫c
seg_stats = summary_ml.groupby('Segment')['CLV_pred_final'].agg(['count','mean','sum']).sort_values('mean')
display(seg_stats)

# Boxplot CLV theo ph√¢n kh√∫c
import seaborn as sns
plt.figure(figsize=(6,4))
sns.boxplot(x='Segment', y='CLV_pred_final', data=summary_ml, palette='coolwarm')
plt.title('Ph√¢n b·ªë CLV d·ª± b√°o theo ph√¢n kh√∫c')
plt.tight_layout(); plt.show()

"""#16.Xu·∫•t CSV k·∫øt qu·∫£ - 3 outputs"""

# --- L∆∞u output ---
OUT_DIR = './clv_outputs_final_xgb'
import os; os.makedirs(OUT_DIR, exist_ok=True)

out_pred = summary_ml[['customer_unique_id','p_alive','CLV_prob_6m',
                       'CLV_actual_6m','CLV_pred_final','Segment']]
out_pred.to_csv(f'{OUT_DIR}/customer_clv_predictions_xgb.csv', index=False)

seg_stats.to_csv(f'{OUT_DIR}/segment_stats_xgb.csv')

imp_df.sort_values(ascending=False).to_csv(f'{OUT_DIR}/xgb_feature_importance.csv')

print("ƒê√£ l∆∞u to√†n b·ªô k·∫øt qu·∫£ v√†o th∆∞ m·ª•c:", OUT_DIR)

"""# Ph·∫ßn 2: ƒê∆∞a ra Insights (d·ª±a theo s·ªë li·ªáu v√† bi·ªÉu ƒë·ªì in ra)"""

# ƒê∆∞·ªùng cong Pareto (Top 30% kh√°ch h√†ng t·∫°o bao nhi√™u % CLV)
srt = summary_ml.sort_values('CLV_pred_final', ascending=False).reset_index(drop=True)
srt['cum_customers'] = (np.arange(len(srt)) + 1) / len(srt)
srt['cum_value'] = srt['CLV_pred_final'].cumsum() / srt['CLV_pred_final'].sum()

plt.figure(figsize=(7,4))
plt.plot(srt['cum_customers'], srt['cum_value'], label='Pareto')
plt.axvline(0.3, color='r', linestyle='--', label='Top 30% KH')
plt.title('ƒê∆∞·ªùng cong Pareto ‚Äî CLV d·ª± b√°o (XGB Final)')
plt.xlabel('% kh√°ch h√†ng t√≠ch l≈©y')
plt.ylabel('% CLV t√≠ch l≈©y')
plt.legend(); plt.tight_layout(); plt.show()

# Top 10 kh√°ch h√†ng c√≥ CLV cao nh·∫•t
top10 = summary_ml.sort_values('CLV_pred_final', ascending=False).head(10)
display(top10[['customer_unique_id','CLV_pred_final','Segment']])

import matplotlib.pyplot as plt
import seaborn as sns

# --- Bar chart t·ªïng CLV theo ph√¢n kh√∫c ---
plt.figure(figsize=(7,4))
sns.barplot(x=seg_stats.index, y='sum', data=seg_stats.reset_index(),
            palette=['#FF9999', '#87CEFA', '#90EE90'])
plt.title('T·ªïng CLV d·ª± b√°o theo ph√¢n kh√∫c kh√°ch h√†ng')
plt.ylabel('T·ªïng CLV (6 th√°ng)')
plt.xlabel('Ph√¢n kh√∫c')

# Hi·ªÉn th·ªã nh√£n gi√° tr·ªã tr√™n ƒë·∫ßu c·ªôt
for i, v in enumerate(seg_stats['sum']):
    plt.text(i, v + seg_stats['sum'].max()*0.02, f"{v:,.0f}", ha='center', fontsize=10, fontweight='bold')

plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# --- Bar chart trung b√¨nh CLV theo ph√¢n kh√∫c ---
plt.figure(figsize=(7,4))
sns.barplot(x=seg_stats.index, y='mean', data=seg_stats.reset_index(),
            palette=['#FF9999', '#87CEFA', '#90EE90'])
plt.title('Gi√° tr·ªã CLV trung b√¨nh theo ph√¢n kh√∫c kh√°ch h√†ng')
plt.ylabel('CLV trung b√¨nh (6 th√°ng)')
plt.xlabel('Ph√¢n kh√∫c')

for i, v in enumerate(seg_stats['mean']):
    plt.text(i, v + seg_stats['mean'].max()*0.03, f"{v:,.0f}", ha='center', fontsize=10, fontweight='bold')

plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# --- Pie chart th·ªÉ hi·ªán t·ªâ tr·ªçng t·ªïng CLV ---
plt.figure(figsize=(6,6))
colors = ['#FF9999', '#87CEFA', '#90EE90']
plt.pie(seg_stats['sum'], labels=seg_stats.index, autopct='%1.1f%%',
        startangle=120, colors=colors, textprops={'fontsize':11})
plt.title('T·ª∑ tr·ªçng t·ªïng CLV d·ª± b√°o theo ph√¢n kh√∫c')
plt.tight_layout()
plt.show()

import pandas as pd, numpy as np, matplotlib.pyplot as plt

# ƒê·ªçc d·ªØ li·ªáu d·ª± b√°o CLV
out_pred = pd.read_csv(f"{OUT_DIR}/customer_clv_predictions_xgb.csv")

# T√≠nh ph√¢n ph·ªëi Pareto
clv = out_pred["CLV_pred_final"].clip(lower=0)
out_pred = out_pred.assign(_clv=clv).sort_values("_clv", ascending=False).reset_index(drop=True)
out_pred["cum_cust_pct"] = (np.arange(len(out_pred)) + 1) / len(out_pred)
out_pred["cum_clv"] = out_pred["_clv"].cumsum()
total_clv = out_pred["_clv"].sum()
out_pred["cum_clv_pct"] = out_pred["cum_clv"] / total_clv

# C√°c m·ªëc % CLV
for t in [0.5, 0.7, 0.8, 0.9]:
    share = out_pred.loc[out_pred["cum_clv_pct"] >= t, "cum_cust_pct"].iloc[0]
    print(f"{int(t*100)}% t·ªïng CLV ƒë·∫°t ƒë∆∞·ª£c b·ªüi {share:.1%} kh√°ch h√†ng")

# Bi·ªÉu ƒë·ªì Pareto
plt.figure(figsize=(6,4))
plt.plot(out_pred["cum_cust_pct"], out_pred["cum_clv_pct"], lw=2, color="#007AFF")
plt.xlabel("% kh√°ch h√†ng (t√≠ch l≈©y)")
plt.ylabel("% CLV d·ª± b√°o (t√≠ch l≈©y)")
plt.title("ƒê∆∞·ªùng cong Pareto ‚Äì Ph√¢n b·ªï CLV d·ª± b√°o (XGBoost)")
plt.grid(True, linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()

# ƒê·ªçc file ph√¢n kh√∫c
seg_stats = pd.read_csv(f"{OUT_DIR}/segment_stats_xgb.csv")
seg_stats.rename(columns=str.lower, inplace=True)

# T√≠nh t·ª∑ l·ªá KH v√† CLV
seg_stats["share_customers"] = seg_stats["count"] / seg_stats["count"].sum()
seg_stats["share_clv"] = seg_stats["sum"] / seg_stats["sum"].sum()

# L√†m tr√≤n & ƒë·ªãnh d·∫°ng b·∫£ng
seg_stats["share_customers_fmt"] = (seg_stats["share_customers"]*100).round(0).astype(int).astype(str).radd("~").add("%")
seg_stats["share_clv_fmt"] = (seg_stats["share_clv"]*100).round(0).astype(int).astype(str).radd("~").add("%")
table = seg_stats[["segment", "share_customers_fmt", "share_clv_fmt"]]
table.columns = ["Ph√¢n kh√∫c", "T·ª∑ l·ªá kh√°ch h√†ng", "T·ª∑ l·ªá CLV"]

print("\nüìä Ph√¢n kh√∫c kh√°ch h√†ng theo CLV:")
print(table.to_string(index=False))

# Bi·ªÉu ƒë·ªì CLV trung b√¨nh
plt.figure(figsize=(6,4))
plt.bar(seg_stats["segment"], seg_stats["mean"], color=["#007AFF","#00C853","#FFAB00"])
plt.title("Gi√° tr·ªã CLV trung b√¨nh theo ph√¢n kh√∫c (Low‚ÄìMid‚ÄìHigh)")
plt.xlabel("Ph√¢n kh√∫c")
plt.ylabel("CLV trung b√¨nh (d·ª± b√°o)")
plt.grid(True, axis="y", linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()

import pandas as pd, numpy as np

# ƒê·ªçc l·∫°i file ƒë·ªÉ c√≥ bi·∫øn out_pred
out_pred = pd.read_csv(f"{OUT_DIR}/customer_clv_predictions_xgb.csv")

# Chu·∫©n b·ªã d·ªØ li·ªáu CLV
clv = out_pred["CLV_pred_final"].clip(lower=0)
out_pred = out_pred.assign(_clv=clv).sort_values("_clv", ascending=False).reset_index(drop=True)
out_pred["cum_cust_pct"] = (np.arange(len(out_pred)) + 1) / len(out_pred)
out_pred["cum_clv"] = out_pred["_clv"].cumsum()
total_clv = out_pred["_clv"].sum()
out_pred["cum_clv_pct"] = out_pred["cum_clv"] / total_clv

# === T·ª´ ƒë√¢y l√† ƒëo·∫°n t√≥m t·∫Øt ===
n_customers = len(out_pred)
mean_clv = out_pred["_clv"].mean()
median_clv = out_pred["_clv"].median()
p95_clv = out_pred["_clv"].quantile(0.95)
share_80pct = out_pred.loc[out_pred["cum_clv_pct"] >= 0.8, "cum_cust_pct"].iloc[0]

print("\nüßæ T√≥m t·∫Øt nhanh insight m√¥ h√¨nh CLV:")
print(f"- S·ªë kh√°ch h√†ng: {n_customers:,}")
print(f"- CLV trung b√¨nh: {mean_clv:,.2f}")
print(f"- CLV trung v·ªã: {median_clv:,.2f}")
print(f"- CLV 95th percentile: {p95_clv:,.2f}")
print(f"- Top {share_80pct*100:.1f}% kh√°ch h√†ng n·∫Øm gi·ªØ 80% t·ªïng CLV d·ª± b√°o.")